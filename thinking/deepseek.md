深度求索（DeepSeek）开源了一系列旨在优化大型语言模型（LLM）训练和推理效率的工具和框架。其中，3FS、FlashMLA、DeepEP 和 DeepGEMM 是其技术栈中的关键组成部分。

3FS (Fire-Flyer File System)
3FS 是一个为人工智能和高性能计算（AI-HPC）工作负载设计的开源并行文件系统 [1][2]。它旨在解决大规模模型训练中数据读取的瓶颈问题 [2][3]。

核心特性：
高性能读取：3FS 优先优化随机读取速度，这对于需要从庞大数据集中持续访问随机训练数据的AI模型训练至关重要 [1]。它通过聚合数千个SSD的吞吐量和数百个存储节点的带宽，实现了极高的读取速度，在一个案例中达到了7.3 TB/s [1]。
分离式架构：它采用计算和存储资源分离的架构，为AI训练节点提供了灵活高效的数据访问方式 [2][3]。
高容错性和一致性：通过链式复制（chain replication）等技术确保数据在分布式系统中的一致性和容错能力 [3][4]。
针对AI优化的设计：与传统文件系统不同，3FS 几乎完全忽略了读取缓存，因为在AI训练中，数据通常只被读取一次，缓存的效用不大 [1]。
FlashMLA (Flash Multi-head Latent Attention)
FlashMLA 是一个专为NVIDIA Hopper架构GPU（如H800）设计的高效多头潜在注意力（Multi-head Latent Attention, MLA）解码内核 [5][6]。MLA是DeepSeek-V2模型中采用的一项创新架构，旨在解决大模型推理过程中的显存瓶颈问题 [7][8]。

核心特性：
优化的解码性能：FlashMLA 专为Transformer模型的自回归解码过程进行优化，显著提升了变长序列的处理效率 [5][6]。
Paged KV Cache：通过使用块大小为64的分页键值（KV）缓存机制，大幅降低了处理长序列时的内存开销 [5][9]。
高精度与高吞吐：支持BF16和FP16等精度格式，在保证计算速度的同时维持了数值稳定性，在H800 SXM5上可实现高达3000 GB/s的内存带宽和580 TFLOPS的计算能力 [5][6]。
降低推理成本：通过显著压缩KV缓存，MLA技术能够大幅降低推理成本并提升生成吞吐量。例如，在DeepSeek-V2中，MLA技术将KV缓存减少了93.3%，并将最大生成吞吐量提升了5.76倍 [7][10]。
DeepEP (Deep Expert Parallelism)
DeepEP 是一个为混合专家模型（Mixture-of-Experts, MoE）和专家并行（Expert Parallelism, EP）场景设计的下一代分布式通信框架 [11][12]。MoE模型通过将计算任务分配给不同的“专家”子网络来提升模型容量和效率，而DeepEP则负责优化这些专家之间的数据通信 [12][13]。

核心特性：
高效的All-to-All通信：提供了高吞吐、低延迟的GPU all-to-all通信内核，完美支持MoE模型中的调度（dispatch）和合并（combine）操作 [11][14]。
支持低精度操作：支持包括FP8在内的低精度数据类型，以进一步提升通信效率 [11][14]。
异构域数据传输优化：专门优化了不同通信域（如NVLink到RDMA）之间的数据传输，确保在训练和推理任务中都能获得卓越性能 [11][14]。
灵活的配置和高并行效率：用户可以根据需求调整配置，并且其独特的基于钩子（hook-based）的通信-计算重叠方法，可以在不占用流多处理器（SM）资源的情况下实现出色的并行效率 [11][13]。
DeepGEMM (Deep General Matrix Multiplication)
DeepGEMM 是一个专注于FP8（8位浮点数）通用矩阵乘法（GEMM）的开源库，旨在为深度学习提供高效、简洁的矩阵运算实现 [15][16]。

核心特性：
专为Hopper架构优化：DeepGEMM 针对NVIDIA Hopper架构的Tensor Cores进行了深度优化，充分利用了其硬件特性 [15][16]。
支持MoE和密集型GEMM：该库不仅支持标准的密集型矩阵乘法，还支持MoE模型中常见的组合矩阵乘法（grouped GEMMs） [15][16]。
高精度与高性能：通过细粒度缩放（fine-grained scaling）技术解决了FP8计算中可能出现的数值溢出或下溢问题，确保了计算的稳定性 [16][17]。
轻量级与易用性：代码库非常简洁，核心内核仅约300行代码，并通过即时编译（Just-In-Time, JIT）技术，无需冗长的编译过程即可轻松集成到现有项目中 [15][18]。
总而言之，这四个开源项目是深度求索为了构建和优化其大型语言模型（如DeepSeek-V2）而开发的核心基础设施 [19][20]。它们分别从数据存储、注意力机制、分布式通信和底层计算等多个层面解决了AI大模型在训练和推理过程中面临的关键挑战，共同构成了一个高效、经济的AI系统解决方案。